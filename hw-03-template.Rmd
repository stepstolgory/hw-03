---
title: "hw-03"
author: "Stepan Mikoyan (S2289431)"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
## **DO NOT EDIT THIS CODE CHUNK**
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(tidymodels)
```


## Data Load

```{r read_data}
gss16<-read.csv("data/gss16.csv")
```

## Data preparation before modelling

The `gss16` data set needs to be prepared prior to modelling. This is not an assessed component to your assignment, but it is an important part of the data science process. Carefully copy the following code into the beginning of your `.Rmd` file and ensure that you understand how the data has been prepared.

### Cleaning and selecting rows 

Create a new data frame called `gss16_advfront` that includes the variables `advfront`, `emailhr` (Number of hours spent on email weekly), `educ` (education level), `polviews` (political views) and `wrkstat` (working status). Remove any row that contains any `NA`s using the `drop_na()` command

```{r}
gss16_advfront <- gss16 %>%
  select(advfront, emailhr, educ, polviews, wrkstat) %>%
  drop_na()
```

### Re-levelling `advfront`

The `advfront` variable contains responses to the question "Even if it brings no immediate benefits, scientific research that advances the frontiers of knowledge is necessary and should be supported by the federal government." The possible answers are on the 5-point Likert scale: `"Strongly agree"`, `"Agree"`, `"Dont know"`, `"Disagree"` and `"Strongly disagree"`. For the purpose of this assignment, the `advfront` variable needs to be transformed such that it has two levels: 

* `"Agree"` - combining the options `"Strongly agree"` and `"Agree"`.
* `"Not agree"` - combining the options `"Dont know"`, `"Disagree"` and `"Strongly disagree"`.

The following code does that this re-levelling:

```{r}
gss16_advfront <- gss16_advfront %>%
  mutate(
    advfront = case_when(
      advfront == "Strongly agree" ~ "Agree",
      advfront == "Agree" ~ "Agree",
      TRUE ~ "Not agree"
    ),
    advfront = fct_relevel(advfront, "Not agree", "Agree")
  )
```


### Re-levelling `polviews`


```{marginfigure}
You can do this in various ways. One option is to use the `str_detect()` function to detect the existence of words like liberal or conservative. Note that these sometimes show up with lowercase first letters and sometimes with upper case first letters. To detect either in the `str_detect()` function, you can use "[Ll]iberal" and "[Cc]onservative". But feel free to solve the problem however you like, this is just one option!
```

The `polviews` variable contains information about the participant's political position within 7 categories ranging from `"Extremely liberal"` to `"Extrmly conservative"`. Here we wish to simplify the range of options to 3 categories - `"Conservative"` , `"Moderate"`, and `"Liberal"`. This is achieved via the following code:

```{r}
gss16_advfront <- gss16_advfront %>%
  mutate(
    polviews = case_when(
      str_detect(polviews, "[Cc]onservative") ~ "Conservative",
      str_detect(polviews, "[Ll]iberal") ~ "Liberal",
      TRUE ~ polviews
    ),
    polviews = fct_relevel(polviews, "Conservative", "Moderate", "Liberal")
  )
```

Please see [chapter 14 of R for Data Science](https://r4ds.had.co.nz/strings.html) for more on string processing commands.

You now have the cleaned and pre-processe data to use for modelling, you can continue with the `gss16_advfront` for the following questions. 

## Exercise 1: Create a linear regression model

Consider the numerical values `educ` and `emailhr` from your new data set `gss16_advfront`.

a) Fit a linear regression model to predicting `emailhr` based on the `educ`. From your output, state the formula for the line-of-best fit and give an interpretation of the `emailhr` estimate.


b) Comment on the overall performance of the linear regression model. Support your statements with an appropriate data visualisation and model fit statistics.

```{r}
email_model <- linear_reg() %>%
  set_engine("lm") %>%
  fit(emailhr~educ, data = gss16_advfront)



email_model_aug <- augment(email_model$fit)

ggplot(data = gss16_advfront, aes(x = educ, y = emailhr)) +
  geom_point() +
  labs(
    title = "Hours on email per week vs. Years in education",
    subtitle = "With the model given by tidymodel",
    x = "Years in education (years)",
    y = "Time spend on email per week (hours)"
  ) +
  stat_function(fun = function(x) 0.6854031*x - 2.7573196)

ggplot(email_model_aug, mapping = aes(x = .fitted, y = .resid)) +
  geom_point(alpha = 0.5) +
  geom_hline(yintercept = 0, color = "gray", lty = "dashed") +
  labs(x = "Predicted time", y = "Residuals")
```


a) 
emailhr = 0.6854031*educ - 2.7573196
For every extra year in education, it is expected for the person to spend, on average, an extra 0.6854031 hours on email per week. If the person has 0 years in education, they are expected to spend an average of -2.7573196 hours.

b)
The linear regression model is not a good representation of the data, this can be seen in the first plot, where the line predicted by the model is not representative of the data, in addition, the residuals plot has a distinct fan shape suggesting that the model is a poor fit.

## Exercise 2: Create a workflow to fit a model

In this part, we're going to build a model to predict whether someone agrees or doesn't agree with the following statement:

Even if it brings no immediate benefits, scientific research that advances the frontiers of knowledge is necessary and should be supported by the federal government.

The responses to the question on the GSS about this statement are in the `advfront` variable, in the `gss16_advfront` data that you obtained.

First, use the following code to split the dataset into a training dataset (`gss16_train`) and a testing dataset (`gss16_test`). This code splits the data into 75\% training and 25\% testing.

```{r split-data}
set.seed(1234)
gss16_split <- initial_split(gss16_advfront)
gss16_train <- training(gss16_split)

gss16_test  <- testing(gss16_split)
```

a) Build a workflow for the training data that consists of a recipe (`gss16_rec_1`) and a model (`gss16_mod_1`). Name this workflow `gss16_wflow_1`.
    
The recipe (named `gss16_rec_1`) should contain the following steps for predicting `advfront` from `educ`:

  - `step_dummy()` to create dummy variables for `all_nominal()` variables that are predictors, i.e. not "outcomes". You can select outcomes using `all_outcomes()`

  - The model (named `gss16_mod_1`) should specify a model that is appropriate for the data as well as the computational engine.

```{r}
gss16_rep_1 <- recipe(
  advfront ~ educ,
  data = gss16_train
  )%>%
  step_dummy(all_nominal_predictors())
summary(gss16_rep_1)

gss16_mod_1 <- logistic_reg() %>% 
  set_engine("glm")

gss16_wflow_1 <-
  workflow() %>%
  add_model(gss16_mod_1) %>%
  add_recipe(gss16_rep_1)




```

b) Explain why you have chosen the model that you have selected.

## Exercise 3: Logistic regression with single predictor

a) Apply the workflow you defined earlier to the training dataset and named the model as `gss16_fit_1`. Display the resulting tibble containing the fitted model parameters. 

```{r}
gss16_fit_1 <- gss16_wflow_1 %>% 
  fit(data = gss16_train)

tidy(gss16_fit_1)
```

b) Use the fitted models to predict the test data, plot the ROC curves for the predictions.

```{r}
gss_pred_1 <- predict(gss16_fit_1, gss16_test, type = "prob") %>% 
  bind_cols(gss16_test)
roc1 <- gss_pred_1 %>%
  roc_curve(
    truth = advfront,
    .pred_Agree,
    event_level = "second"
  )
autoplot(roc1)

gss_pred_1 %>%
    roc_auc(
    truth = advfront,
    .pred_Agree,
    event_level = "second"
  )
```

## Exercise 4: Logistic regression modelling and interpretation

We are now going to model `advfront` using the explanatory variables `polviews`, `wrkstat`, and `educ`.

a) Build a new workflow for the training data that consists of a recipe (`gss16_rec_2`) and a model (`gss16_mod_2`). Name this workflow `gss16_wflow_2`. You can simply **copy, paste and edit** the code from earlier.
    
Now the new recipe (named `gss16_rec_2`) should contain the followings for predicting `advfront` from `polviews`, `wrkstat`, and `educ`:

  - `step_dummy()` to create dummy variables for `all_nominal()` variables that are predictors, i.e. not "outcomes". You can select outcomes using `all_outcomes()`

  - The model (named `gss16_mod_2`) should specify a model that is appropriate for the data as well as the computational engine.
  
Apply the new workflow to the training dataset and create a new model fit 
named as `gss16_fit_2`. 

Then use the fitted models to predict the test data, plot the ROC curve for the predictions for both models, and calculate the areas under the ROC curves.

```{r}
gss16_rep_2 <- recipe(
  advfront ~ .,
  data = gss16_train
  )%>%
  step_rm(emailhr) %>%
  step_dummy(all_nominal(), - all_outcomes())

gss16_mod_2 <- logistic_reg() %>% 
  set_engine("glm")

gss16_wflow_2 <- workflow() %>% 
  add_model(gss16_mod_2) %>% 
  add_recipe(gss16_rep_2)

gss16_fit_2 <- gss16_wflow_2 %>% 
  fit(data = gss16_train)

tidy(gss16_fit_2) %>% print(n = 30)

```

b) Comment on which model performs better 

  * the model only including `educ`, as model 1 (`gss16_fit_1`) 
  * the model including `polviews`, `wrkstat`, and `educ` with `gss16_split` as model 2 (`gss16_fit_2`)
  
Explain your reasoning.

```{r}
library(patchwork)
gss_pred_2 <- predict(gss16_fit_2, gss16_test, type = "prob") %>% 
  bind_cols(gss16_test)

roc2 <- gss_pred_2 %>%
  roc_curve(
    truth = advfront,
    .pred_Agree,
    event_level = "second"
  )

gss_pred_2 %>%
    roc_auc(
    truth = advfront,
    .pred_Agree,
    event_level = "second"
  )

ggplot(mapping = aes(x = 1-specificity, y = sensitivity)) +
  geom_path(data = fortify(roc1, melt = FALSE), aes(color = "roc for fit 1")) + 
  geom_path(data = fortify(roc2, melt = FALSE), aes(color = "roc for fit 2")) + 
  xlab("1-specificity") + ylab("sensitivity")+
  labs(color = NULL)+
  geom_abline(lty = 3) 
```


b) 
gss_fit_1 performs better than gss_fit_2. This can be seen from the overlaying plot and the respective areas under the curves. The area under the gss_fit_1 curve is greater than the area under the gss_fit_2 suggesting that it is a better fit. However, both are not very good at predicting the response, as every prediction returns "Agree".


